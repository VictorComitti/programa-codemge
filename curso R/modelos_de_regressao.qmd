---
title: "Modelagem em R"
format: html
editor: visual
author: Victor Schmidt Comitti
self-contained: true
toc: true
---

# Regress√£o Linear

A regress√£o linear √© um dos modelos estat√≠sticos mais simples e amplamente utilizados em **Data Science** para entender a rela√ß√£o entre vari√°veis. Neste cap√≠tulo, abordaremos os conceitos fundamentais da regress√£o linear, com foco em exemplos pr√°ticos usando a linguagem **R**.

## Conceitos B√°sicos

### Defini√ß√£o

A regress√£o linear simples √© usada para modelar a rela√ß√£o entre duas vari√°veis quantitativas: - **Vari√°vel dependente** (ou resposta): O que estamos tentando prever (Y). - **Vari√°vel independente** (ou explicativa): O que usamos para prever (X).

O modelo √© representado pela equa√ß√£o:

```{=tex}
\begin{equation}
Y = \beta_0 + \beta_1X + \epsilon
\end{equation}
```
Onde:

-   $\beta_0$ √© o intercepto;
-   $\beta_1$ √© o coeficiente angular (inclina√ß√£o);
-   $\epsilon \sim N(0, \sigma^{2})$ √© o termo de erro.

### Objetivo

O objetivo da regress√£o linear √© estimar os par√¢metros $\beta_0$ e $\beta_1$ de forma que a soma dos quadrados dos res√≠duos (SSR)  seja minimizada, ou seja, encontrar a reta que melhor se ajusta aos dados.

```{r, echo = FALSE, warning = FALSE, message = FALSE}
library(ggplot2)
library(gganimate)
library(dplyr)
library(tidyr)
library(patchwork)
library(car)
library(tidyverse)
library(tseries)
library(broom)
library(MASS)
library(ggpubr)

# Gerando dados fict√≠cios com menos pontos
set.seed(123)
n <- 15  # Reduzi o n√∫mero de pontos
X <- rnorm(n, mean = 5, sd = 2)
Y <- 2 * X + rnorm(n)

dados <- data.frame(X, Y)

# Ajustando o modelo de regress√£o
modelo <- lm(Y ~ X, data = dados)

# Coeficientes da reta de m√≠nimos quadrados
intercepto_final <- coef(modelo)[1]
coef_final <- coef(modelo)[2]

# Ponto de rota√ß√£o: centro dos dados
X_centro <- mean(X)
Y_centro <- mean(Y)

# Inclina√ß√µes intermedi√°rias para simular a rota√ß√£o
slopes <- seq(0, coef_final, length.out = 20)
intercepts <- seq(Y_centro, intercepto_final, length.out = 20)  # Variando o intercepto

# Calculando o SSR para cada passo
SSR_list <- sapply(1:length(slopes), function(i) {
  intercept_atual <- intercepts[i]
  slope_atual <- slopes[i]

  # Previs√£o dos valores de Y para cada reta
  Y_pred <- intercept_atual + slope_atual * X

  # Somat√≥rio dos quadrados dos res√≠duos
  SSR <- sum((Y - Y_pred)^2)

  return(SSR)
})

# Criando os gr√°ficos com diferentes interceptos e inclina√ß√µes
graficos_df <- bind_rows(lapply(1:length(slopes), function(i) {
  intercept_atual <- intercepts[i]
  slope_atual <- slopes[i]

  # Previs√£o dos valores de Y para cada reta
  Y_pred <- intercept_atual + slope_atual * X

  # Criando um dataframe para armazenar os pontos, interceptos, inclina√ß√µes e SSR
  data.frame(X, Y, Y_pred,
             intercept = intercept_atual,
             slope = slope_atual,
             SSR = SSR_list[i],
             iter = i)
}))

# Plot da anima√ß√£o do gr√°fico de dispers√£o com res√≠duos e reta ajustada
p1 <- ggplot(graficos_df, aes(x = X, y = Y)) +
  geom_point(size = 4) +  # Aumentando o tamanho dos pontos
  geom_abline(aes(intercept = intercept, slope = slope), color = "blue") +
  geom_segment(aes(xend = X, yend = Y_pred), linetype = "dashed", color = "red") +
  labs(title = "Reta de Regress√£o") +
  transition_manual(iter) +
  ease_aes('linear')

# Plot do somat√≥rio dos quadrados dos res√≠duos (SSR) ao longo das itera√ß√µes
p2 <- ggplot(graficos_df, aes(x = iter, y = SSR)) +
  geom_line() +
  geom_point(size = 4) +
  labs(title = "Somat√≥rio dos Quadrados dos Res√≠duos (SSR)",
       x = "Itera√ß√£o", y = "SSR") +
  transition_manual(iter) +
  ease_aes('linear')

# Criando e salvando as anima√ß√µes separadamente
anim1 <- animate(p1, nframes = length(slopes), fps = 2)
anim2 <- animate(p2, nframes = length(slopes), fps = 2)

```

<div style="display: flex;">

  <div style="flex: 50%; padding-right: 10px;">

```{r, echo = FALSE}
  anim1
```

</div> <div style="flex: 50%; padding-left: 10px;">

```{r, echo = FALSE}
anim2
```
</div> </div>

### Premissas do Modelo de Regress√£o Linear

### 1. Linearidade

A rela√ß√£o entre a vari√°vel dependente  $Y$ e as vari√°veis independentes $X$ deve ser linear. Isso significa que os par√¢metros do modelo (intercepto e coeficientes) devem descrever uma rela√ß√£o linear entre $X$ e $Y$.


### 2. Independ√™ncia dos Erros

Os erros devem ser independentes entre si. Isso significa que o erro de uma observa√ß√£o n√£o deve influenciar o erro de outra observa√ß√£o. Uma viola√ß√£o dessa premissa pode resultar em autocorrela√ß√£o, que √© comum em dados com depend√™ncia temporal. 

### 3. Homocedasticidade

Os res√≠duos devem ter vari√¢ncia constante ao longo de todos os n√≠veis da vari√°vel explicativa $X$. Se a vari√¢ncia dos res√≠duos aumentar ou diminuir sistematicamente √† medida que os valores de $X$ mudam, dizemos que h√° heterocedasticidade. A heterocedasticidade afeta a estima√ß√£o da vari√¢ncia o que leva a problemas nos c√°lculos de testes de hip√≥tese e intervalos de confian√ßa.  

```{r, echo = TRUE}
# Carregando o pacote ggplot2
library(ggplot2)

# Gerando dados heteroced√°sticos
set.seed(123)
n <- 100
x <- seq(1, 10, length.out = n)
y <- 2 * x + rnorm(n, mean = 0, sd = x)

# Criando um data frame com os dados
data <- data.frame(x = x, y = y)

# Criando o gr√°fico com ggplot2
ggplot(data, aes(x = x, y = y)) +
  geom_point(color = "blue") +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  labs(title = "Exemplo de Dados Heteroced√°sticos",
       x = "Vari√°vel Independente (x)",
       y = "Vari√°vel Dependente (y)") +
  theme_minimal()

```

### 4. Normalidade dos erros

Os res√≠duos devem seguir uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia constante. Isso √© necess√°rio para garantir a validade dos testes de hip√≥teses e intervalos de confian√ßa. A normalidade dos erros pode ser verificada por meio de gr√°ficos de probabilidade normal (Q-Q plot) ou testes estat√≠sticos, como o Shapiro-Wilk ou o Jarque Bera.

### 5. Aus√™ncia de Multicolinearidade

Em modelos de regress√£o m√∫ltipla, as vari√°veis independentes n√£o devem estar altamente correlacionadas entre si. Se houver uma forte correla√ß√£o entre duas ou mais vari√°veis explicativas, pode ocorrer multicolinearidade, o que dificulta a interpreta√ß√£o dos coeficientes individuais.

Uma maneira comum de diagnosticar multicolinearidade √© calcular o Fator de Infla√ß√£o da Vari√¢ncia (VIF) para as vari√°veis explicativas. Valores de VIF superiores a 10 sugerem multicolinearidade problem√°tica.

## Exemplo Pr√°tico em R

Vamos usar o dataset `mtcars` para ilustrar a regress√£o linear simples. Neste exemplo, queremos prever o consumo de combust√≠vel (mpg) com base no peso do carro (wt).

```{r, echo = TRUE}

library(tidyverse)
# Carregar os dados
data(mtcars)

# Visualiza√ß√£o inicial dos dados
ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point() +
  labs(title = "Rela√ß√£o entre Peso e Consumo de Combust√≠vel",
       x = "Peso do Carro (1000 lbs)",
       y = "Consumo (mpg)") +
  theme_minimal()

# Ajustar o modelo de regress√£o linear
modelo <- lm(mpg ~ wt, data = mtcars)

# Exibir o resumo do modelo
summary(modelo)

# Visualizar reta de regress√£o

ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point() +  # Gr√°fico de dispers√£o
  geom_smooth(method = "lm", se = FALSE, color = "blue") +  # Adiciona a reta de regress√£o
  labs(title = "Rela√ß√£o entre Peso e Consumo de Combust√≠vel",
       x = "Peso do Carro (1000 lbs)",
       y = "Consumo (mpg)") +
  theme_minimal()
```
### Interpreta√ß√£o dos coeficientes

O intercepto $\beta_0$ representa o valor previsto de \(Y\) quando a vari√°vel $X$ √© igual a 0. Em alguns casos, pode n√£o ser interpret√°vel diretamente se o valor $X = 0$ n√£o fizer sentido no contexto dos dados.

O coeficiente angular $\beta_1$ representa a **taxa de varia√ß√£o** da vari√°vel $Y$ para cada unidade adicional de $X$. Em outras palavras, indica quanto $Y$ aumenta (ou diminui) para cada aumento de uma unidade em $X$.

**Exemplo:** Se $\beta_1 = -5$ em um modelo que relaciona o peso do carro com o consumo de combust√≠vel, isso significa que, para cada aumento de 1 unidade no peso do carro (1.000 lbs), espera-se que o consumo de combust√≠vel diminua, em m√©dia, 5 milhas por gal√£o.


## Previs√£o em Modelo de Regress√£o 

A previs√£o em um modelo de regress√£o linear simples envolve o uso da equa√ß√£o ajustada para prever o valor da vari√°vel dependente $Y$ com base em novos valores da vari√°vel independente $X$. A equa√ß√£o de um modelo de regress√£o simples pode ser expressa como:

\begin{equation}
\hat{Y} = \beta_0 + \beta_1 X
\end{equation}

Onde:

 - $\hat{Y}$ √© o valor previsto de $Y$ (a vari√°vel resposta),
 - $\beta_0$ √© o intercepto (valor previsto de $Y$ quando $X = 0$,
 - $\beta_1$ √© o coeficiente angular, indicando a varia√ß√£o esperada em $Y$ para cada unidade de aumento em $X$,
 - $X$ √© a vari√°vel independente (preditor).

### Exemplo de Previs√£o em R

Ap√≥s ajustar o modelo de regress√£o, √© poss√≠vel usar a fun√ß√£o `predict()` para realizar previs√µes:

```{r}
# Ajustar o modelo de regress√£o linear

# Fazer previs√µes para novos dados
novos_dados <- data.frame(wt = c(2.5, 3.0, 4.0))
previsoes <- predict(modelo, newdata = novos_dados)

previsoes

```

## An√°lise dos Res√≠duos e M√©tricas de Diagn√≥stico

A an√°lise dos res√≠duos √© uma etapa crucial para avaliar a qualidade do ajuste do modelo de regress√£o. Os res√≠duos representam a diferen√ßa entre os valores observados e os valores ajustados (\(\hat{Y}\)):

\begin{equation}
\text{Res√≠duo} = Y_i - \hat{Y}_i
\end{equation}

Essa an√°lise nos ajuda a verificar se o modelo de regress√£o est√° capturando adequadamente a rela√ß√£o entre as vari√°veis e se as premissas do modelo s√£o atendidas.

### Hip√≥teses sobre os Res√≠duos

1. **Independ√™ncia**: Os res√≠duos devem ser independentes entre si. N√£o deve haver padr√£o ou correla√ß√£o entre os res√≠duos ao longo do tempo ou de uma vari√°vel.
   
2. **M√©dia zero**: A m√©dia dos res√≠duos deve ser pr√≥xima de zero. Caso contr√°rio, isso pode indicar que o modelo est√° mal ajustado.

3. **Homocedasticidade (vari√¢ncia constante)**: Os res√≠duos devem apresentar vari√¢ncia constante ao longo dos valores previstos. Isso significa que a dispers√£o dos res√≠duos n√£o deve aumentar ou diminuir sistematicamente √† medida que os valores previstos $\hat{Y}$ mudam.


4. **Distribui√ß√£o normal**: Os res√≠duos devem seguir uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia constante. Esta premissa √© importante principalmente para infer√™ncias estat√≠sticas (p-valores, intervalos de confian√ßa).

### Verificando as Premissas

#### Gr√°fico dos Res√≠duos vs Valores Ajustados

Um gr√°fico dos res√≠duos em fun√ß√£o dos valores ajustados $\hat{Y}$ √© √∫til para verificar a homocedasticidade e a m√©dia zero dos res√≠duos. Se as premissas forem atendidas, os res√≠duos devem se distribuir aleatoriamente ao redor da linha zero.

```{r, echo=FALSE}
# Gerando gr√°fico de res√≠duos vs valores ajustados
modelo <- lm(mpg ~ wt, data = mtcars)
residuos <- residuals(modelo)
valores_ajustados <- fitted(modelo)

ggplot(data = data.frame(residuos, valores_ajustados), aes(x = valores_ajustados, y = residuos)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Res√≠duos vs Valores Ajustados",
       x = "Valores Ajustados",
       y = "Res√≠duos")
```

Um histograma dos res√≠duos pode ser utilizado para verificar a hip√≥tese de normalidade. 

```{r}
ggplot(data = data.frame(residuos), aes(x = residuos)) +
  geom_histogram(binwidth = 1, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Histograma dos Res√≠duos",
       x = "Res√≠duos",
       y = "Frequ√™ncia")
```

O gr√°fico QQ-Plot compara a distribui√ß√£o dos res√≠duos com uma distribui√ß√£o normal te√≥rica. Se os res√≠duos estiverem normalmente distribu√≠dos, os pontos se alinhar√£o pr√≥ximos a uma linha reta.

```{r}
qqnorm(residuos)
qqline(residuos, col = "red")
```

Al√©m de gr√°ficos como o histograma e o QQ-Plot, tamb√©m podemos utilizar testes estat√≠sticos para verificar a normalidade dos res√≠duos. Dois testes amplamente utilizados s√£o o **Teste de Jarque-Bera** e o **Teste de Shapiro-Wilk**.

#### Teste de Jarque-Bera

O teste de Jarque-Bera verifica se os res√≠duos t√™m curtose e assimetria compat√≠veis com uma distribui√ß√£o normal. A hip√≥tese nula do teste √© que os res√≠duos seguem uma distribui√ß√£o normal. Se o p-valor for menor que o n√≠vel de signific√¢ncia escolhido (geralmente $\alpha = 0.05 $, rejeitamos a hip√≥tese nula, indicando que os res√≠duos n√£o s√£o normalmente distribu√≠dos.

```{r}
library(tseries)

jarque.bera.test(residuos)
```

#### Teste de Shapiro-Wilk

O teste de Shapiro-Wilk tamb√©m verifica a normalidade dos res√≠duos. A hip√≥tese nula √© que os dados s√£o normalmente distribu√≠dos. Um p-valor pequeno indica que os res√≠duos n√£o seguem uma distribui√ß√£o normal.

```{r}
shapiro.test(residuos)
```

### M√©tricas de Diagn√≥stico

Al√©m dos gr√°ficos, tamb√©m podemos usar m√©tricas quantitativas para avaliar o desempenho do modelo:

#### R¬≤ (R-quadrado)

Mede a propor√ß√£o da variabilidade em $Y$ que √© explicada pela variabilidade em $X$. O valor de $R^2$ varia entre 0 e 1. Um $R^2$ mais pr√≥ximo de 1 indica que o modelo explica bem a variabilidade dos dados.

```{r}
summary(modelo)$r.squared
```


#### RSE (Erro Padr√£o Residual): 

Mede o desvio padr√£o dos res√≠duos. Um RSE menor indica um melhor ajuste do modelo.

```{r}
summary(modelo)$sigma
```


#### p-valores:

Testam a signific√¢ncia estat√≠stica dos coeficientes. Se os p-valores forem menores que um n√≠vel de signific√¢ncia (ùõº= 0.05), rejeitamos a hip√≥tese nula de que os coeficientes s√£o zero, indicando que existe uma rela√ß√£o significativa entreùëãeùëå.

```{r}
summary(modelo)$coefficients
```

#### AIC/BIC 

Avaliam a qualidade do modelo ajustado penalizando o n√∫mero de par√¢metros. Modelos com menor AIC ou BIC s√£o prefer√≠veis.

```{r}
AIC(modelo)
BIC(modelo)
```

### Nota sobre o R¬≤


O $R^2$ √© amplamente utilizado como medida de diagn√≥stico em modelos lineares, mas sua interpreta√ß√£o pode ser enganosa em diversas situa√ß√µes. Modelos corretos podem apresentar $R^2$ baixo, enquanto modelos inadequados podem exibir $R^2$ elevado. Al√©m disso, o $R^2$ tende a aumentar sempre que novos preditores s√£o adicionados ao modelo, independentemente de sua relev√¢ncia. Uma alternativa mais robusta √© o $R^2$ ajustado, que s√≥ aumenta quando a nova vari√°vel realmente contribui para melhorar o poder explicativo do modelo

## Regress√£o M√∫ltipla

A regress√£o m√∫ltipla √© uma extens√£o da regress√£o linear simples que permite modelar a rela√ß√£o entre uma vari√°vel dependente $Y$ e v√°rias vari√°veis independentes $X_1, X_2, ..., X_n$. Em vez de ter apenas uma vari√°vel explicativa, o modelo de regress√£o m√∫ltipla considera m√∫ltiplos fatores simultaneamente, oferecendo uma vis√£o mais detalhada sobre os dados e a rela√ß√£o entre as vari√°veis.

A forma geral do modelo de regress√£o m√∫ltipla √©:

\begin{equation}
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_n X_n + \varepsilon
\end{equation}

Onde:

 - $Y$ √© a vari√°vel dependente (resposta);
 - $X_1, X_2, ..., X_n$ s√£o as vari√°veis independentes (explicativas);
 - $\beta_0$ √© o intercepto, ou seja, o valor esperado de $Y$ quando todas as vari√°veis $X_i$ s√£o iguais a zero;
 - $\beta_1, \beta_2, ..., \beta_n$ s√£o os coeficientes de regress√£o associados a cada $X_i$, representando a mudan√ßa esperada em $Y$ para uma varia√ß√£o unit√°ria em $X_i$, mantendo as demais vari√°veis constantes;
 - $\varepsilon$ √© o termo de erro aleat√≥rio, representando a variabilidade em $Y$ n√£o explicada pelas vari√°veis $X_i$.

#### Interpreta√ß√£o dos Coeficientes na Regress√£o M√∫ltipla

Cada coeficiente $\beta_i$ em uma regress√£o m√∫ltipla tem uma interpreta√ß√£o similar √† regress√£o linear simples, mas com uma diferen√ßa fundamental: ele captura o impacto marginal de $X_i$ sobre $Y$, **mantendo as demais vari√°veis independentes constantes**. Isso significa que os coeficientes de um modelo de regress√£o m√∫ltipla controlam o efeito de outras vari√°veis explicativas, permitindo uma an√°lise mais precisa dos efeitos individuais.

**Aten√ß√£o:** A rela√ß√£o entre as vari√°veis explicativas e a vari√°vel resposta, ainda que estatisticamente significativas, **n√£o implicam em causalidade**. No entanto, o modelo de regress√£o simples pode ter interpreta√ß√£o causal contanto que certas hip√≥teses sejam atendidas (ver back-door criterion).

#### Exemplo:

Suponha que estamos tentando prever o pre√ßo de uma casa $Y $ com base em v√°rias caracter√≠sticas, como tamanho da casa em metros quadrados $X_1$, n√∫mero de quartos $X_2$ e idade da casa $X_3$. O modelo de regress√£o m√∫ltipla poderia ser escrito da seguinte forma:

\begin{equation}
Pre√ßo = \beta_0 + \beta_1 (Tamanho) + \beta_2 (Quartos) + \beta_3 (Idade) + \varepsilon
\end{equation}

## Exemplo em R

Vamos retomar o banco de dados 'mtcars' e testar duas especifica√ß√µes de modelos: 

 - **Modelo 1:** Vamos prever o consumo de combust√≠vel (mpg) usando o peso (wt), pot√™ncia (hp), n√∫mero de cilindros (cyl) e tempo de acelera√ß√£o de 1/4 de milha (qsec).

 - **Modelo 2:** Vamos prever o consumo de combust√≠vel (mpg) usando o peso (wt), pot√™ncia (hp), n√∫mero de cilindros (cyl) e o eixo traseiro (drat).

```{r}
modelo1 <- lm(mpg ~ wt + hp + cyl + qsec, 
              data = mtcars)
modelo2 <- lm(mpg ~ wt + hp + cyl + drat, 
              data = mtcars)
```

### Resultados 

#### Modelo 1

```{r}
summary(modelo1)
```

Apenas o intercepto e a vari√°vel 'wt' s√£o estatisticamente signficativas. Vamos olhar os res√≠duos para avaliar a qualidade do ajuste. 

```{r}
diagnostico_residuos <- function(modelo) {
  
  # Res√≠duos e ajustes
  residuos <- residuals(modelo)
  ajustes <- fitted(modelo)
  
  # Gr√°fico de res√≠duos vs ajustes
  plot_residuos <- ggplot(data.frame(ajustes, residuos), aes(x = ajustes, y = residuos)) +
    geom_point() +
    geom_hline(yintercept = 0, color = "red") +
    labs(title = "Res√≠duos vs Ajustes", x = "Ajustes", y = "Res√≠duos")
  
  # Histograma dos res√≠duos
  hist_residuos <- ggplot(data.frame(residuos), aes(x = residuos)) +
    geom_histogram(binwidth = 0.5, fill = "blue", color = "black") +
    labs(title = "Histograma dos Res√≠duos", x = "Res√≠duos", y = "Frequ√™ncia")
  
  # QQ-plot dos res√≠duos
  qqplot_residuos <- ggqqplot(residuos, title = "QQ-plot dos Res√≠duos", color = "blue")
  
  # Teste de normalidade dos res√≠duos - Shapiro-Wilk
  shapiro_test <- shapiro.test(residuos)
  
  # Teste de normalidade dos res√≠duos - Jarque-Bera
  jarque_bera_test <- jarque.bera.test(residuos)
  
  # Retornar os gr√°ficos e os testes como uma lista
  return(list(
    plot_residuos = plot_residuos,
    hist_residuos = hist_residuos,
    qqplot_residuos = qqplot_residuos,
    shapiro_test = shapiro_test,
    jarque_bera_test = jarque_bera_test
  ))
}

# Diagn√≥stico para Modelo 1
diagnostico_residuos(modelo1)

```

A um n√≠vel de signific√¢ncia de 5% **n√£o rejeitamos** a hip√≥tese nula de que os res√≠duos do modelo de regress√£o s√£o normalmente distribu√≠dos.

#### Modelo 2

```{r}
summary(modelo2)
```
Novamente, apenas o intercepto e a vari√°vel 'wt' s√£o estatisticamente significativas. O $R^2$ e $R^2$ ajustado de ambos os modelos s√£o muito semelhantes. Para avaliar a qualidade do ajuste vamos examinar os res√≠duos do modelo 2. 

```{r}
diagnostico_residuos(modelo2)
```

Dessa vez, o resultado do teste de Shapiro sugere que os res√≠duos n√£o s√£o normais. Uma maneira de comparar os dois modelos diretamente √© atrav√©s do AIC e BIC.

```{r}
metricas_diagnostico <- c(AIC(modelo1), BIC(modelo1), 
                          AIC(modelo2), BIC(modelo2))
names(metricas_diagnostico) <- c("AIC model1", "BIC model1", "AIC model2", "BIC model2")

metricas_diagnostico
```
Pelas m√©tricas de diagn√≥stico, o modelo 2 √© levemente superior ao modelo 1. 

### Multicolinearidade

Para finalizar a an√°lise vamos utilizar a fun√ß√£o VIF do pacote 'car' para testar se h√° multicolinearidade entre as vari√°veis. 

```{r}
vif(modelo1)
vif(modelo2)
```
Em ambos os modelos a vari√°vel 'cyl' aparece com um valor de VIF acima de 5, o que indica a possibilidade de que esse preditor √© altamente correlacionado com outras vari√°veis do modelo.

